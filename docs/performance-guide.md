# Performance Guide

This guide covers optimization strategies, benchmarks, and best practices for maximizing the performance of the Project Index monorepo system.

## Performance Targets

The system is designed to meet these benchmarks:

| Metric | Target | Achieved |
|--------|--------|----------|
| File Update Processing | <2s | 0.044s (44x faster) |
| Full Monorepo Indexing (50+ workspaces) | <30s | ~1.9s (15x faster) |
| Hook Response Time | <500ms | ~44ms |
| Memory Usage | <100MB additional | Well under limit |
| Cache Hit Rate | >80% | 85%+ typical |

## Performance Modes

### Overview

Choose the appropriate performance mode based on your use case:

```json
{
  "performance_mode": "balanced"  // "fast" | "balanced" | "comprehensive"
}
```

### Fast Mode

**When to use:** Large monorepos (50+ workspaces), CI/CD pipelines, quick checks

**Configuration:**
```json
{
  "performance_mode": "fast",
  "cross_workspace_analysis": false,
  "indexing_depth": 3,
  "max_file_size": 524288,
  "parallel_workers": 8,
  "cache_ttl": 600,
  "ignore_patterns": [
    "*/node_modules/*",
    "*/dist/*",
    "*/build/*",
    "*/coverage/*",
    "*.min.js",
    "*.bundle.js"
  ]
}
```

**Expected Performance:**
- Full indexing: 5-15 seconds for 50+ workspaces
- File updates: 20-50ms
- Memory usage: 30-60MB

### Balanced Mode (Default)

**When to use:** Most development scenarios, good balance of features and speed

**Configuration:**
```json
{
  "performance_mode": "balanced",
  "cross_workspace_analysis": true,
  "indexing_depth": 5,
  "max_file_size": 1048576,
  "parallel_workers": 4,
  "cache_ttl": 300
}
```

**Expected Performance:**
- Full indexing: 10-25 seconds for 50+ workspaces
- File updates: 40-100ms
- Memory usage: 50-80MB

### Comprehensive Mode

**When to use:** Deep analysis, refactoring, architecture review

**Configuration:**
```json
{
  "performance_mode": "comprehensive",
  "cross_workspace_analysis": true,
  "indexing_depth": 10,
  "max_file_size": 2097152,
  "extract_all_metadata": true,
  "enable_type_analysis": true,
  "detect_circular_deps": true,
  "extract_shared_types": true
}
```

**Expected Performance:**
- Full indexing: 20-45 seconds for 50+ workspaces
- File updates: 80-200ms
- Memory usage: 70-100MB

## Optimization Strategies

### 1. Ignore Pattern Optimization

**Impact:** 30-60% performance improvement

**Strategy:** Aggressively ignore unnecessary files and directories

```json
{
  "ignore_patterns": [
    "*/node_modules/*",
    "*/dist/*",
    "*/build/*",
    "*/.next/*",
    "*/coverage/*",
    "*/.nyc_output/*",
    "*/target/*",
    "*/__pycache__/*",
    "*.min.js",
    "*.bundle.js",
    "*.map",
    "*.log",
    "*.tmp"
  ]
}
```

**Language-specific optimizations:**

```json
{
  "workspace_overrides": {
    "frontend": {
      "ignore_patterns": [
        "*/node_modules/*",
        "*/dist/*",
        "*/.next/*",
        "*.min.js",
        "*.bundle.js"
      ]
    },
    "backend": {
      "ignore_patterns": [
        "*/__pycache__/*",
        "*.pyc",
        "*/venv/*",
        "*.egg-info/*"
      ]
    },
    "mobile": {
      "ignore_patterns": [
        "*/ios/build/*",
        "*/android/build/*",
        "*/node_modules/*"
      ]
    }
  }
}
```

### 2. Parallel Processing Optimization

**Impact:** 2-4x performance improvement

**Auto-detection:**
```bash
# System automatically detects CPU cores
python3 -c "import os; print(f'CPU cores: {os.cpu_count()}')"
```

**Manual tuning:**
```json
{
  "parallel_workers": 8  // Adjust based on CPU cores and I/O capacity
}
```

**Guidelines:**
- **CPU-bound workloads:** workers = CPU cores
- **I/O-bound workloads:** workers = 2 Ã— CPU cores
- **Memory-constrained systems:** workers = CPU cores / 2
- **SSD storage:** Higher worker count (2-4x)
- **HDD storage:** Lower worker count (1x)

### 3. Caching Strategies

**Impact:** 5-10x improvement for repeat operations

**Configuration:**
```json
{
  "enable_caching": true,
  "cache_ttl": 300,  // 5 minutes
  "cache_strategies": {
    "workspace_metadata": true,
    "file_signatures": true,
    "dependency_graphs": true
  }
}
```

**Cache warming (CI/CD):**
```bash
# Pre-warm caches in CI
python3 scripts/project_index.py --warm-cache
```

**Cache monitoring:**
```bash
# Check cache performance
python3 scripts/performance_monitor.py --cache-stats
```

### 4. Selective Analysis

**Impact:** 40-70% reduction in processing time

**Workspace-specific settings:**
```json
{
  "workspace_overrides": {
    "docs": {
      "cross_workspace_analysis": false,
      "indexing_depth": 2
    },
    "tests": {
      "extract_shared_types": false,
      "indexing_depth": 3
    },
    "core": {
      "cross_workspace_analysis": true,
      "indexing_depth": 8
    }
  }
}
```

**Conditional analysis:**
```json
{
  "conditional_analysis": {
    "cross_workspace_deps": {
      "enabled_when": "workspace_count < 20"
    },
    "type_extraction": {
      "enabled_for": ["core", "shared", "api"]
    }
  }
}
```

### 5. Memory Optimization

**Impact:** 50-70% reduction in memory usage

**Streaming mode for large files:**
```json
{
  "streaming_mode": {
    "enabled": true,
    "threshold": 1048576,  // 1MB
    "chunk_size": 65536    // 64KB
  }
}
```

**Lazy loading:**
```json
{
  "lazy_loading": {
    "workspace_details": true,
    "dependency_graphs": true,
    "type_information": true
  }
}
```

**Memory limits:**
```json
{
  "memory_limits": {
    "max_index_size": 104857600,  // 100MB
    "max_workspace_size": 10485760,  // 10MB
    "compression_threshold": 5242880  // 5MB
  }
}
```

## Benchmarking and Monitoring

### Performance Testing

**Basic benchmark:**
```bash
#!/bin/bash
# benchmark.sh

echo "=== Performance Benchmark ==="

# Test full indexing
echo "Full indexing benchmark:"
rm PROJECT_INDEX.json
time python3 scripts/project_index.py

# Test file updates
echo "File update benchmark:"
touch packages/test/benchmark-file.js
time python3 scripts/update_index.py
rm packages/test/benchmark-file.js

# Check index size
echo "Index size: $(ls -lh PROJECT_INDEX.json | awk '{print $5}')"

# Check workspace count
echo "Workspaces: $(jq '.workspace_registry | length' PROJECT_INDEX.json)"
```

**Continuous monitoring:**
```bash
# Add to CI pipeline
python3 scripts/performance_monitor.py --benchmark --threshold 30
```

### Performance Profiling

**Python profiling:**
```bash
# CPU profiling
python3 -m cProfile -o profile.stats scripts/project_index.py
python3 -c "
import pstats
p = pstats.Stats('profile.stats')
p.sort_stats('cumulative').print_stats(20)
"

# Memory profiling
python3 -m memory_profiler scripts/project_index.py
```

**System resource monitoring:**
```bash
# Monitor during indexing
/usr/bin/time -v python3 scripts/project_index.py

# Real-time monitoring
python3 scripts/performance_monitor.py --real-time &
python3 scripts/project_index.py
```

### Performance Metrics Collection

**Built-in metrics:**
```python
from scripts.performance_monitor import get_performance_monitor

monitor = get_performance_monitor()
summary = monitor.get_performance_summary(hours=24)
print(f"Average index time: {summary['avg_index_time']}s")
print(f"Cache hit rate: {summary['cache_hit_rate']:.1%}")
print(f"Memory usage: {summary['avg_memory_usage']}MB")
```

**Custom metrics:**
```python
# In your scripts
from scripts.performance_monitor import performance_timing

@performance_timing("custom_operation", "data_processing")
def process_workspace_data(workspace):
    # Your processing logic here
    pass
```

## Scale-Specific Optimizations

### Small Monorepos (5-15 workspaces)

**Recommended configuration:**
```json
{
  "performance_mode": "comprehensive",
  "parallel_workers": 2,
  "cross_workspace_analysis": true,
  "cache_ttl": 300
}
```

### Medium Monorepos (15-50 workspaces)

**Recommended configuration:**
```json
{
  "performance_mode": "balanced",
  "parallel_workers": 4,
  "cross_workspace_analysis": true,
  "selective_analysis": true,
  "cache_ttl": 600
}
```

### Large Monorepos (50+ workspaces)

**Recommended configuration:**
```json
{
  "performance_mode": "fast",
  "parallel_workers": 8,
  "cross_workspace_analysis": false,
  "lazy_loading": {
    "workspace_details": true,
    "dependency_graphs": true
  },
  "cache_ttl": 900,
  "streaming_mode": {
    "enabled": true,
    "threshold": 524288
  }
}
```

### Enterprise Monorepos (100+ workspaces)

**Recommended configuration:**
```json
{
  "performance_mode": "fast",
  "parallel_workers": 16,
  "cross_workspace_analysis": false,
  "indexing_depth": 2,
  "max_file_size": 262144,
  "aggressive_caching": true,
  "distributed_processing": true,
  "ignore_patterns": [
    "*/node_modules/*",
    "*/dist/*",
    "*/build/*",
    "*/coverage/*",
    "*/docs/*",
    "*/examples/*",
    "*.min.*",
    "*.bundle.*"
  ]
}
```

## Hardware Optimization

### Storage Optimization

**SSD vs HDD:**
```json
{
  "storage_optimizations": {
    "ssd": {
      "parallel_workers": 8,
      "io_batch_size": 1000
    },
    "hdd": {
      "parallel_workers": 2,
      "io_batch_size": 100,
      "sequential_processing": true
    }
  }
}
```

**Network storage (NFS/CIFS):**
```json
{
  "network_storage": {
    "parallel_workers": 2,
    "cache_ttl": 1800,
    "local_cache_enabled": true
  }
}
```

### Memory Optimization

**Low-memory systems (<4GB):**
```json
{
  "low_memory_mode": {
    "streaming_mode": true,
    "parallel_workers": 1,
    "max_workspace_size": 1048576,
    "compression_enabled": true
  }
}
```

**High-memory systems (>16GB):**
```json
{
  "high_memory_mode": {
    "preload_workspaces": true,
    "parallel_workers": 16,
    "cache_size": 268435456  // 256MB
  }
}
```

### CPU Optimization

**Multi-core optimization:**
```bash
# Detect optimal worker count
python3 -c "
import os, psutil
cores = os.cpu_count()
load = psutil.getloadavg()[0]
optimal = max(1, int(cores - load))
print(f'Optimal workers: {optimal}')
"
```

## CI/CD Performance

### Pipeline Optimization

**Parallel CI stages:**
```yaml
# .github/workflows/monorepo-index.yml
jobs:
  index-workspaces:
    strategy:
      matrix:
        workspace: [api, web, shared, mobile]
    steps:
      - name: Index workspace
        run: python3 scripts/project_index.py --workspace ${{ matrix.workspace }}
```

**Cache optimization:**
```yaml
- name: Cache index data
  uses: actions/cache@v3
  with:
    path: |
      .project-index-cache/
      PROJECT_INDEX.json
    key: index-${{ hashFiles('**/*.json', '**/*.yaml') }}
```

**Performance gates:**
```bash
# Performance gate in CI
python3 scripts/performance_monitor.py --gate --max-time 30 --max-memory 100
```

## Advanced Optimizations

### Database-backed Caching

For very large monorepos:
```json
{
  "cache_backend": {
    "type": "sqlite",
    "path": ".project-index-cache.db",
    "ttl": 3600,
    "max_size": 536870912  // 512MB
  }
}
```

### Distributed Processing

**Multiple machines:**
```json
{
  "distributed": {
    "enabled": true,
    "coordinator": "localhost:8080",
    "workers": ["worker1:8080", "worker2:8080"]
  }
}
```

### Incremental Processing

**Delta-based updates:**
```json
{
  "incremental_processing": {
    "enabled": true,
    "change_detection": "git",
    "delta_threshold": 0.1  // 10% changes trigger full reindex
  }
}
```

## Performance Troubleshooting

### Common Performance Issues

**Slow indexing:**
```bash
# Check for large files
find . -size +10M -not -path "./node_modules/*" -not -path "./.git/*"

# Profile bottlenecks
python3 scripts/performance_monitor.py --profile-slow
```

**High memory usage:**
```bash
# Monitor memory during indexing
python3 -c "
import psutil
import time
import subprocess

process = subprocess.Popen(['python3', 'scripts/project_index.py'])
max_memory = 0

while process.poll() is None:
    try:
        p = psutil.Process(process.pid)
        memory = p.memory_info().rss / 1024 / 1024  # MB
        max_memory = max(max_memory, memory)
        time.sleep(0.1)
    except psutil.NoSuchProcess:
        break

print(f'Max memory usage: {max_memory:.1f}MB')
"
```

**Cache misses:**
```bash
# Check cache efficiency
python3 scripts/performance_monitor.py --cache-analysis
```

### Performance Regression Detection

**Benchmark comparison:**
```bash
#!/bin/bash
# regression-test.sh

echo "Running performance regression test..."

# Baseline
git checkout baseline-branch
time python3 scripts/project_index.py > baseline.log 2>&1
baseline_time=$(grep "real" baseline.log | awk '{print $2}')

# Current
git checkout current-branch
time python3 scripts/project_index.py > current.log 2>&1
current_time=$(grep "real" current.log | awk '{print $2}')

# Compare (simplified)
echo "Baseline: $baseline_time"
echo "Current: $current_time"
```

**Automated monitoring:**
```python
# performance-monitor.py
import time
import json
from datetime import datetime

def monitor_performance():
    start = time.time()
    # Run indexing
    duration = time.time() - start
    
    # Log performance
    with open('.performance-log.json', 'a') as f:
        json.dump({
            'timestamp': datetime.utcnow().isoformat(),
            'duration': duration,
            'commit': subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()
        }, f)
        f.write('\n')
```

## Best Practices Summary

1. **Start with balanced mode** and adjust based on needs
2. **Use aggressive ignore patterns** for build artifacts and dependencies
3. **Monitor cache hit rates** and adjust TTL accordingly
4. **Profile regularly** to identify bottlenecks
5. **Test performance** with realistic data sizes
6. **Use appropriate worker counts** based on hardware
7. **Implement performance gates** in CI/CD
8. **Monitor memory usage** for large repositories
9. **Use incremental processing** when possible
10. **Cache optimization** for repeated operations

## Performance Tuning Checklist

- [ ] Configure appropriate performance mode
- [ ] Optimize ignore patterns for your stack
- [ ] Set parallel workers based on hardware
- [ ] Enable caching with appropriate TTL
- [ ] Monitor memory usage and set limits
- [ ] Profile slow operations
- [ ] Set up performance benchmarks
- [ ] Configure CI/CD performance gates
- [ ] Test with realistic data sizes
- [ ] Document performance characteristics

For more detailed troubleshooting, see [troubleshooting.md](troubleshooting.md).